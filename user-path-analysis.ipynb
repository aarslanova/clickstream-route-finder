{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks, code quality and readability (no single-letter variables, PEP8 compliant) etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-10-08 17:30:36,737 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, ShortType\n",
    "\n",
    "sc = SparkContext(appName=\"ClickstreamAnalysis\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"user_id\", ShortType(), True),\n",
    "    StructField(\"session_id\", ShortType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_page\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"hdfs:/data/clickstream.csv\", header=True, sep=\"\\t\", schema=schema)\n",
    "df.show(5)\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Using Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "\n",
    "def pretty_print(top_routes):\n",
    "    print(\"+--------------------+-----+\")\n",
    "    print(\"|               route|count|\")\n",
    "    print(\"+--------------------+-----+\")\n",
    "    \n",
    "    for route, count in top_routes:\n",
    "        truncated_route = (route[:17] + '...') if len(route) > 20 else route.ljust(20, ' ')\n",
    "        aligned_count = str(count).rjust(5, ' ')\n",
    "        print(f\"|{truncated_route}|{aligned_count}|\")\n",
    "    \n",
    "    print(\"+--------------------+-----+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_func(row):\n",
    "    try:\n",
    "        error_flag = 1 if 'error' in row.event_type else 0\n",
    "        return [((row.user_id, row.session_id), [(row.timestamp, error_flag, row.event_page)])]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in map_func: {str(e)}, with row: {row}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((562, 507), [(1695584127, 0, 'main')]),\n",
       " ((562, 507), [(1695584134, 0, 'main')]),\n",
       " ((562, 507), [(1695584144, 0, 'main')]),\n",
       " ((562, 507), [(1695584147, 0, 'main')]),\n",
       " ((562, 507), [(1695584154, 1, 'main')])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_rdd = df_rdd.flatMap(map_func)\n",
    "mapped_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((1974, 700),\n",
       "  [(1696091444, 0, 'main'),\n",
       "   (1696091654, 0, 'main'),\n",
       "   (1696091785, 0, 'online'),\n",
       "   (1696092968, 0, 'main'),\n",
       "   (1696093073, 0, 'main'),\n",
       "   (1696093508, 0, 'online'),\n",
       "   (1696093548, 0, 'rabota'),\n",
       "   (1696093620, 0, 'bonus'),\n",
       "   (1696093655, 0, 'bonus'),\n",
       "   (1696093851, 0, 'bonus'),\n",
       "   (1696093918, 0, 'archive'),\n",
       "   (1696093967, 0, 'archive'),\n",
       "   (1696094104, 0, 'tariffs'),\n",
       "   (1696094159, 0, 'bonus'),\n",
       "   (1696094341, 0, 'bonus'),\n",
       "   (1696094925, 0, 'vklad'),\n",
       "   (1696095152, 0, 'vklad'),\n",
       "   (1696095430, 0, 'vklad'),\n",
       "   (1696096365, 0, 'news'),\n",
       "   (1696096405, 0, 'news'),\n",
       "   (1696267341, 0, 'main'),\n",
       "   (1696267408, 0, 'main'),\n",
       "   (1696267991, 0, 'bonus'),\n",
       "   (1696268484, 0, 'online'),\n",
       "   (1696268499, 0, 'rabota'),\n",
       "   (1696269064, 0, 'rabota'),\n",
       "   (1696270284, 0, 'rabota'),\n",
       "   (1696270671, 0, 'archive'),\n",
       "   (1696270827, 0, 'archive'),\n",
       "   (1696271112, 0, 'archive'),\n",
       "   (1696271830, 0, 'archive'),\n",
       "   (1696271959, 0, 'tariffs'),\n",
       "   (1696272019, 0, 'tariffs'),\n",
       "   (1696272428, 0, 'tariffs'),\n",
       "   (1696272595, 0, 'tariffs'),\n",
       "   (1696272766, 0, 'bonus'),\n",
       "   (1696273019, 0, 'bonus'),\n",
       "   (1696273021, 0, 'bonus'),\n",
       "   (1696274035, 0, 'bonus'),\n",
       "   (1696274108, 0, 'bonus'),\n",
       "   (1696274751, 0, 'rabota'),\n",
       "   (1696275058, 0, 'rabota'),\n",
       "   (1696276067, 0, 'internet'),\n",
       "   (1696277135, 0, 'internet'),\n",
       "   (1696277499, 0, 'vklad'),\n",
       "   (1696277896, 0, 'archive'),\n",
       "   (1696278333, 0, 'archive'),\n",
       "   (1696278697, 0, 'archive'),\n",
       "   (1696279104, 0, 'archive'),\n",
       "   (1696280120, 0, 'rabota')])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_sorted_rdd = mapped_rdd.reduceByKey(lambda a,b: a+b).mapValues(lambda v: sorted(v, key=lambda x: x[0]))\n",
    "grouped_sorted_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((2209, 541),\n",
       "  [(1695585442, 0, 'main'),\n",
       "   (1695585480, 0, 'main'),\n",
       "   (1695585553, 0, 'bonus'),\n",
       "   (1695585571, 0, 'bonus'),\n",
       "   (1695585593, 0, 'online'),\n",
       "   (1695585610, 0, 'online'),\n",
       "   (1695585733, 0, 'online'),\n",
       "   (1695585783, 0, 'online'),\n",
       "   (1695585792, 0, 'internet'),\n",
       "   (1695585812, 0, 'internet'),\n",
       "   (1695585822, 0, 'internet'),\n",
       "   (1695585842, 0, 'internet'),\n",
       "   (1695585878, 0, 'news'),\n",
       "   (1695585918, 0, 'news'),\n",
       "   (1695585934, 0, 'news'),\n",
       "   (1695585951, 0, 'news'),\n",
       "   (1695585994, 0, 'main'),\n",
       "   (1695586015, 0, 'main')])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cumsum_errors(events_list):\n",
    "    cumsum = 0\n",
    "    new_list = []\n",
    "    for event in events_list:\n",
    "        timestamp, error_flag, event_page = event\n",
    "        cumsum += error_flag\n",
    "        new_list.append((timestamp, cumsum, event_page))\n",
    "    return new_list\n",
    "\n",
    "cumsum_rdd = grouped_sorted_rdd.mapValues(cumsum_errors)\n",
    "cumsum_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1974, 700),\n",
       "  [(1696270671, 0, 'archive'),\n",
       "   (1696270827, 0, 'archive'),\n",
       "   (1696271112, 0, 'archive'),\n",
       "   (1696271830, 0, 'archive'),\n",
       "   (1696271959, 0, 'tariffs'),\n",
       "   (1696272019, 0, 'tariffs'),\n",
       "   (1696272428, 0, 'tariffs'),\n",
       "   (1696272595, 0, 'tariffs'),\n",
       "   (1696272766, 0, 'bonus'),\n",
       "   (1696273019, 0, 'bonus'),\n",
       "   (1696273021, 0, 'bonus'),\n",
       "   (1696274035, 0, 'bonus'),\n",
       "   (1696274108, 0, 'bonus'),\n",
       "   (1696274751, 0, 'rabota'),\n",
       "   (1696275058, 0, 'rabota'),\n",
       "   (1696276067, 0, 'internet'),\n",
       "   (1696277135, 0, 'internet'),\n",
       "   (1696277499, 0, 'vklad'),\n",
       "   (1696277896, 0, 'archive'),\n",
       "   (1696278333, 0, 'archive'),\n",
       "   (1696278697, 0, 'archive'),\n",
       "   (1696279104, 0, 'archive'),\n",
       "   (1696280120, 0, 'rabota'),\n",
       "   (1696091444, 0, 'main'),\n",
       "   (1696091654, 0, 'main'),\n",
       "   (1696091785, 0, 'online'),\n",
       "   (1696092968, 0, 'main'),\n",
       "   (1696093073, 0, 'main'),\n",
       "   (1696093508, 0, 'online'),\n",
       "   (1696093548, 0, 'rabota'),\n",
       "   (1696093620, 0, 'bonus'),\n",
       "   (1696093655, 0, 'bonus'),\n",
       "   (1696093851, 0, 'bonus'),\n",
       "   (1696093918, 0, 'archive'),\n",
       "   (1696093967, 0, 'archive'),\n",
       "   (1696094104, 0, 'tariffs'),\n",
       "   (1696094159, 0, 'bonus'),\n",
       "   (1696094341, 0, 'bonus'),\n",
       "   (1696094925, 0, 'vklad'),\n",
       "   (1696095152, 0, 'vklad'),\n",
       "   (1696095430, 0, 'vklad'),\n",
       "   (1696096365, 0, 'news'),\n",
       "   (1696096405, 0, 'news'),\n",
       "   (1696267341, 0, 'main'),\n",
       "   (1696267408, 0, 'main'),\n",
       "   (1696267991, 0, 'bonus'),\n",
       "   (1696268484, 0, 'online'),\n",
       "   (1696268499, 0, 'rabota'),\n",
       "   (1696269064, 0, 'rabota'),\n",
       "   (1696270284, 0, 'rabota')])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def contains_error(events_list):\n",
    "    for _, error_flag, _ in events_list:\n",
    "        if error_flag == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "no_error_rdd = cumsum_rdd.filter(lambda x: not contains_error(x[1]))\n",
    "no_error_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((1974, 700),\n",
       "  [(1696270671, 0, 'archive'),\n",
       "   (1696270827, 0, 'archive'),\n",
       "   (1696271112, 0, 'archive'),\n",
       "   (1696271830, 0, 'archive'),\n",
       "   (1696271959, 0, 'tariffs'),\n",
       "   (1696272019, 0, 'tariffs'),\n",
       "   (1696272428, 0, 'tariffs'),\n",
       "   (1696272595, 0, 'tariffs'),\n",
       "   (1696272766, 0, 'bonus'),\n",
       "   (1696273019, 0, 'bonus'),\n",
       "   (1696273021, 0, 'bonus'),\n",
       "   (1696274035, 0, 'bonus'),\n",
       "   (1696274108, 0, 'bonus'),\n",
       "   (1696274751, 0, 'rabota'),\n",
       "   (1696275058, 0, 'rabota'),\n",
       "   (1696276067, 0, 'internet'),\n",
       "   (1696277135, 0, 'internet'),\n",
       "   (1696277499, 0, 'vklad'),\n",
       "   (1696277896, 0, 'archive'),\n",
       "   (1696278333, 0, 'archive'),\n",
       "   (1696278697, 0, 'archive'),\n",
       "   (1696279104, 0, 'archive'),\n",
       "   (1696280120, 0, 'rabota'),\n",
       "   (1696091444, 0, 'main'),\n",
       "   (1696091654, 0, 'main'),\n",
       "   (1696091785, 0, 'online'),\n",
       "   (1696092968, 0, 'main'),\n",
       "   (1696093073, 0, 'main'),\n",
       "   (1696093508, 0, 'online'),\n",
       "   (1696093548, 0, 'rabota'),\n",
       "   (1696093620, 0, 'bonus'),\n",
       "   (1696093655, 0, 'bonus'),\n",
       "   (1696093851, 0, 'bonus'),\n",
       "   (1696093918, 0, 'archive'),\n",
       "   (1696093967, 0, 'archive'),\n",
       "   (1696094104, 0, 'tariffs'),\n",
       "   (1696094159, 0, 'bonus'),\n",
       "   (1696094341, 0, 'bonus'),\n",
       "   (1696094925, 0, 'vklad'),\n",
       "   (1696095152, 0, 'vklad'),\n",
       "   (1696095430, 0, 'vklad'),\n",
       "   (1696096365, 0, 'news'),\n",
       "   (1696096405, 0, 'news'),\n",
       "   (1696267341, 0, 'main'),\n",
       "   (1696267408, 0, 'main'),\n",
       "   (1696267991, 0, 'bonus'),\n",
       "   (1696268484, 0, 'online'),\n",
       "   (1696268499, 0, 'rabota'),\n",
       "   (1696269064, 0, 'rabota'),\n",
       "   (1696270284, 0, 'rabota')])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_func(x, y):\n",
    "    try:\n",
    "        return x + y\n",
    "    except Exception as e:\n",
    "        print(f\"Error in reduce_func: {str(e)}, with x: {x}, y: {y}\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "cumsum_rdd = mapped_rdd.reduceByKey(reduce_func)\n",
    "cumsum_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2209, 541),\n",
       "  [(1695585442, 0, 'main'),\n",
       "   (1695585480, 0, 'main'),\n",
       "   (1695585553, 0, 'bonus'),\n",
       "   (1695585571, 0, 'bonus'),\n",
       "   (1695585593, 0, 'online'),\n",
       "   (1695585610, 0, 'online'),\n",
       "   (1695585733, 0, 'online'),\n",
       "   (1695585783, 0, 'online'),\n",
       "   (1695585792, 0, 'internet'),\n",
       "   (1695585812, 0, 'internet'),\n",
       "   (1695585822, 0, 'internet'),\n",
       "   (1695585842, 0, 'internet'),\n",
       "   (1695585878, 0, 'news'),\n",
       "   (1695585918, 0, 'news'),\n",
       "   (1695585934, 0, 'news'),\n",
       "   (1695585951, 0, 'news'),\n",
       "   (1695585994, 0, 'main'),\n",
       "   (1695586015, 0, 'main')])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def contains_error(events_list):\n",
    "    for _, error_flag, _ in events_list:\n",
    "        if error_flag == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "no_error_rdd = cumsum_rdd.filter(lambda x: not contains_error(x[1]))\n",
    "no_error_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('main-tariffs-rabota-news', 18)]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_counts_rdd = routes_rdd.reduceByKey(lambda x, y: x + y)\n",
    "route_counts_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_routes_rdd = route_counts_rdd.takeOrdered(30, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|main                | 5778|\n",
      "|main-archive        |  845|\n",
      "|main-rabota         |  799|\n",
      "|main-internet       |  694|\n",
      "|main-bonus          |  643|\n",
      "|main-news           |  597|\n",
      "|main-tariffs        |  510|\n",
      "|main-online         |  439|\n",
      "|main-vklad          |  387|\n",
      "|main-archive-rabota |  156|\n",
      "|main-rabota-archive |  153|\n",
      "|main-bonus-rabota   |  134|\n",
      "|main-internet-rabota|  130|\n",
      "|main-news-rabota    |  127|\n",
      "|main-internet-arc...|  127|\n",
      "|main-archive-news   |  124|\n",
      "|main-rabota-internet|  122|\n",
      "|main-archive-inte...|  122|\n",
      "|main-rabota-news    |  121|\n",
      "|main-archive-bonus  |  119|\n",
      "|main-rabota-bonus   |  119|\n",
      "|main-bonus-archive  |  119|\n",
      "|main-news-archive   |  109|\n",
      "|main-internet-bonus |  103|\n",
      "|main-internet-news  |   99|\n",
      "|main-archive-online |   98|\n",
      "|main-archive-tariffs|   97|\n",
      "|main-tariffs-inte...|   93|\n",
      "|main-tariffs-archive|   90|\n",
      "|main-news-internet  |   90|\n",
      "+--------------------+-----+\n"
     ]
    }
   ],
   "source": [
    "pretty_print(top_routes_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 389:============================================>          (12 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 2392|\n",
      "|        main-archive|  337|\n",
      "|         main-rabota|  315|\n",
      "|       main-internet|  265|\n",
      "|          main-bonus|  247|\n",
      "|           main-news|  234|\n",
      "|         main-online|  179|\n",
      "|        main-tariffs|  178|\n",
      "|          main-vklad|  165|\n",
      "| main-archive-rabota|   53|\n",
      "|main-internet-arc...|   50|\n",
      "| main-rabota-archive|   43|\n",
      "|main-archive-inte...|   42|\n",
      "|   main-archive-news|   42|\n",
      "|main-internet-rabota|   41|\n",
      "|  main-bonus-archive|   40|\n",
      "|   main-rabota-bonus|   39|\n",
      "|  main-archive-bonus|   38|\n",
      "|   main-bonus-rabota|   38|\n",
      "|main-rabota-internet|   37|\n",
      "|main-archive-tariffs|   35|\n",
      "|    main-news-rabota|   34|\n",
      "|     main-news-bonus|   33|\n",
      "|main-tariffs-inte...|   32|\n",
      "|  main-internet-news|   32|\n",
      "|main-tariffs-archive|   32|\n",
      "|        main-digital|   31|\n",
      "| main-internet-bonus|   31|\n",
      "|    main-rabota-news|   29|\n",
      "| main-archive-online|   28|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "WITH\n",
    "    EXTRACT_ERRORS\n",
    "AS\n",
    "(\n",
    "    SELECT\n",
    "        *,\n",
    "        SUM(CASE WHEN event_type LIKE '%error%' THEN 1 ELSE 0 END) \n",
    "            OVER window_spec AS cumsum_error\n",
    "    FROM df\n",
    "    WINDOW window_spec AS (\n",
    "        PARTITION BY user_id, session_id\n",
    "        ORDER BY timestamp\n",
    "    )\n",
    "),\n",
    "    MAX_TIMESTAMP\n",
    "AS\n",
    "(\n",
    "    SELECT\n",
    "        user_id\n",
    "        , session_id\n",
    "        , MAX(timestamp) AS timestamp\n",
    "    FROM df\n",
    "    GROUP BY user_id, session_id\n",
    "),\n",
    "    WINDOWED_ROUTES\n",
    "AS\n",
    "(\n",
    "    SELECT\n",
    "        user_id\n",
    "        , session_id\n",
    "        , timestamp\n",
    "        , COLLECT_LIST(event_page) OVER window_spec AS route\n",
    "    FROM EXTRACT_ERRORS\n",
    "    WHERE\n",
    "        cumsum_error = 0\n",
    "        AND event_type = 'page'\n",
    "    WINDOW window_spec AS (\n",
    "        PARTITION BY user_id, session_id\n",
    "        ORDER BY timestamp\n",
    "    )\n",
    ")\n",
    "SELECT\n",
    "    CONCAT_WS('-', W.route) AS route\n",
    "    , COUNT(*) AS count\n",
    "FROM\n",
    "    WINDOWED_ROUTES W\n",
    "JOIN\n",
    "    MAX_TIMESTAMP M\n",
    "ON\n",
    "    W.user_id = M.user_id\n",
    "    AND W.session_id = M.session_id\n",
    "    AND W.timestamp = M.timestamp\n",
    "GROUP BY\n",
    "    W.route\n",
    "ORDER BY\n",
    "    count DESC\n",
    "LIMIT 30\n",
    "\n",
    "\"\"\"\n",
    ").show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Using Spark DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 292:===================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 2392|\n",
      "|        main-archive|  337|\n",
      "|         main-rabota|  315|\n",
      "|       main-internet|  265|\n",
      "|          main-bonus|  247|\n",
      "|           main-news|  234|\n",
      "|         main-online|  179|\n",
      "|        main-tariffs|  178|\n",
      "|          main-vklad|  165|\n",
      "| main-archive-rabota|   53|\n",
      "|main-internet-arc...|   50|\n",
      "| main-rabota-archive|   43|\n",
      "|   main-archive-news|   42|\n",
      "|main-archive-inte...|   42|\n",
      "|main-internet-rabota|   41|\n",
      "|  main-bonus-archive|   40|\n",
      "|   main-rabota-bonus|   39|\n",
      "|  main-archive-bonus|   38|\n",
      "|   main-bonus-rabota|   38|\n",
      "|main-rabota-internet|   37|\n",
      "|main-archive-tariffs|   35|\n",
      "|    main-news-rabota|   34|\n",
      "|     main-news-bonus|   33|\n",
      "|main-tariffs-archive|   32|\n",
      "|  main-internet-news|   32|\n",
      "|main-tariffs-inte...|   32|\n",
      "|        main-digital|   31|\n",
      "| main-internet-bonus|   31|\n",
      "|    main-rabota-news|   29|\n",
      "| main-archive-online|   28|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(*[\"user_id\", \"session_id\"]).orderBy(\"timestamp\")\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn(\"error_flag\", F.when(F.col(\"event_type\").contains(\"error\"), 1).otherwise(0))\n",
    "    .withColumn(\"cumsum_error\", F.sum(\"error_flag\").over(window_spec))\n",
    "    .filter(F.col(\"cumsum_error\") == 0)\n",
    "    .filter(F.col(\"event_type\")==\"page\")\n",
    "    .withColumn(\"route\", F.collect_list(\"event_page\").over(window_spec))\n",
    "    .join(\n",
    "        F.broadcast(\n",
    "            df\n",
    "            .groupBy(*[\"user_id\", \"session_id\"])\n",
    "            .agg(F.max(F.col(\"timestamp\")).alias(\"timestamp\"))\n",
    "        ),\n",
    "        on=[\"user_id\", \"session_id\", \"timestamp\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .withColumn(\"route\", F.concat_ws(\"-\", \"route\"))\n",
    "    .groupBy(\"route\")\n",
    "    .count()\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    "    .limit(30)\n",
    "    .show(30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 19:21:27,934 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 73.0 (TID 664) (475c97847897 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 3122, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 3122, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 540, in func\n",
      "    >>> rdd = sc.parallelize(range(500), 1)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2147, in combineLocally\n",
      "    return merger.items()\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in <lambda>\n",
      "  File \"/tmp/ipykernel_60421/2024702353.py\", line -1, in remove_consecutive_duplicates\n",
      "TypeError: sequence item 0: expected str instance, tuple found\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-10-05 19:21:27,989 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 73.0 (TID 663) (475c97847897 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000002/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2970, in pipeline_func\n",
      "    def _is_barrier(self):\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2970, in pipeline_func\n",
      "    def _is_barrier(self):\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 645, in func\n",
      "    return fraction + numStDev * sqrt(fraction / total)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2147, in combineLocally\n",
      "    return merger.items()\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000002/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000002/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in <lambda>\n",
      "  File \"/tmp/ipykernel_60421/2024702353.py\", line -1, in remove_consecutive_duplicates\n",
      "TypeError: sequence item 0: expected str instance, tuple found\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-10-05 19:21:28,180 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 73.0 (TID 665) (475c97847897 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 607, in func\n",
      "    # this shouldn't happen often because we use a big multiplier for their initial size.\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2147, in combineLocally\n",
      "    return merger.items()\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in <lambda>\n",
      "  File \"/tmp/ipykernel_60421/2024702353.py\", line -1, in remove_consecutive_duplicates\n",
      "TypeError: sequence item 0: expected str instance, tuple found\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-10-05 19:21:28,319 WARN scheduler.TaskSetManager: Lost task 1.1 in stage 73.0 (TID 666) (475c97847897 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000002/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000002/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 607, in func\n",
      "    # this shouldn't happen often because we use a big multiplier for their initial size.\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2147, in combineLocally\n",
      "    return merger.items()\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000002/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n",
      "    for k, v in iterator:\n",
      "  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000002/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in <lambda>\n",
      "  File \"/tmp/ipykernel_60421/2024702353.py\", line -1, in remove_consecutive_duplicates\n",
      "TypeError: sequence item 0: expected str instance, tuple found\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-10-05 19:21:29,149 ERROR scheduler.TaskSetManager: Task 1 in stage 73.0 failed 4 times; aborting job\n",
      "2023-10-05 19:21:29,224 WARN scheduler.TaskSetManager: Lost task 2.3 in stage 73.0 (TID 673) (475c97847897 executor 1): TaskKilled (Stage cancelled)\n",
      "2023-10-05 19:21:29,227 WARN scheduler.TaskSetManager: Lost task 0.3 in stage 73.0 (TID 674) (475c97847897 executor 2): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 73.0 failed 4 times, most recent failure: Lost task 1.3 in stage 73.0 (TID 672) (475c97847897 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 607, in func\n    # this shouldn't happen often because we use a big multiplier for their initial size.\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2147, in combineLocally\n    return merger.items()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in <lambda>\n  File \"/tmp/ipykernel_60421/2024702353.py\", line -1, in remove_consecutive_duplicates\nTypeError: sequence item 0: expected str instance, tuple found\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 607, in func\n    # this shouldn't happen often because we use a big multiplier for their initial size.\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2147, in combineLocally\n    return merger.items()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in <lambda>\n  File \"/tmp/ipykernel_60421/2024702353.py\", line -1, in remove_consecutive_duplicates\nTypeError: sequence item 0: expected str instance, tuple found\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rdd_top_routes \u001b[38;5;241m=\u001b[39m \u001b[43mrdd_routes_count\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtakeOrdered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1508\u001b[0m, in \u001b[0;36mRDD.takeOrdered\u001b[0;34m(self, num, key)\u001b[0m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(a, b):\n\u001b[1;32m   1506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m heapq\u001b[38;5;241m.\u001b[39mnsmallest(num, a \u001b[38;5;241m+\u001b[39m b, key)\n\u001b[0;32m-> 1508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mheapq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsmallest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:999\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m--> 999\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:950\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 950\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 73.0 failed 4 times, most recent failure: Lost task 1.3 in stage 73.0 (TID 672) (475c97847897 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 607, in func\n    # this shouldn't happen often because we use a big multiplier for their initial size.\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2147, in combineLocally\n    return merger.items()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in <lambda>\n  File \"/tmp/ipykernel_60421/2024702353.py\", line -1, in remove_consecutive_duplicates\nTypeError: sequence item 0: expected str instance, tuple found\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/worker.py\", line 609, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in pipeline_func\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 607, in func\n    # this shouldn't happen often because we use a big multiplier for their initial size.\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2147, in combineLocally\n    return merger.items()\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/tmp/hadoop-jovyan/nm-local-dir/usercache/jovyan/appcache/application_1696475076700_0002/container_1696475076700_0002_01_000003/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line -1, in <lambda>\n  File \"/tmp/ipykernel_60421/2024702353.py\", line -1, in remove_consecutive_duplicates\nTypeError: sequence item 0: expected str instance, tuple found\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rdd_top_routes = rdd_routes_count.takeOrdered(20, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|tariffs-main-onli...|    1|\n",
      "|rabota-main-archi...|    1|\n",
      "|archive-rabota-ar...|    1|\n",
      "|news-online-bonus...|    1|\n",
      "|news-main-news-ra...|    1|\n",
      "|online-news-main-...|    1|\n",
      "|news-archive-vkla...|    1|\n",
      "|rabota-main-bonus...|    1|\n",
      "|bonus-archive-mai...|    1|\n",
      "|internet-archive-...|    1|\n",
      "|internet-rabota-m...|    1|\n",
      "|archive-online-ta...|    1|\n",
      "|bonus-news-rabota...|    1|\n",
      "|archive-internet-...|    1|\n",
      "|news-vklad-news-m...|    1|\n",
      "|bonus-archive-vkl...|    1|\n",
      "|archive-main-arch...|    1|\n",
      "|tariffs-bonus-vkl...|    1|\n",
      "|rabota-archive-di...|    1|\n",
      "|bonus-rabota-main...|    1|\n",
      "+--------------------+-----+\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(top_routes):\n",
    "    print(\"+--------------------+-----+\")\n",
    "    print(\"|               route|count|\")\n",
    "    print(\"+--------------------+-----+\")\n",
    "    \n",
    "    for route, count in top_routes:\n",
    "        truncated_route = (route[:17] + '...') if len(route) > 20 else route.ljust(20, ' ')\n",
    "        aligned_count = str(count).rjust(5, ' ')\n",
    "        print(f\"|{truncated_route}|{aligned_count}|\")\n",
    "    \n",
    "    print(\"+--------------------+-----+\")\n",
    "\n",
    "    \n",
    "pretty_print(rdd_top_routes)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
